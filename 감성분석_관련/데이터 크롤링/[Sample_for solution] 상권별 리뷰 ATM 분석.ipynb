{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:35.852073Z",
     "start_time": "2021-10-18T08:06:35.006806Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 불필요한 경고 표시 생략\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "a = %pwd # 현재 경로 a에 할당\n",
    "os.chdir(a) # 파일 로드 경로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:42.396105Z",
     "start_time": "2021-10-18T08:06:35.854069Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('상권별_리뷰.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T04:47:52.632342Z",
     "start_time": "2021-10-18T04:47:51.486408Z"
    }
   },
   "source": [
    "merged_df = pd.DataFrame() # 빈 데이터 프레임 생성\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DataFrame 통합\n",
    "\"\"\"\n",
    "for file in os.listdir(\"review_sample\"): # file이 에너지 사용량 예측 폴더 내에 있는 파일명을 순회\n",
    "    # csv확장자가 있는 경우만 불러오도록 \n",
    "    df = pd.read_csv(\"review_sample/\" + file) #파일들이 있는 폴더의 위치를 명시해준다.\n",
    "    # 행 기준(axis=0)으로 빈 DataFrame과 불러온 DataFrame을 병합 + index는 새로 생성\n",
    "    merged_df = pd.concat([merged_df, df], axis = 0, ignore_index = True)\n",
    "\n",
    "\"\"\"\n",
    "행단위로 붙일 경우 주로 ignore_index = True로 한다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:42.442979Z",
     "start_time": "2021-10-18T08:06:42.399098Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review내용 간단한 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:42.457939Z",
     "start_time": "2021-10-18T08:06:42.445972Z"
    }
   },
   "outputs": [],
   "source": [
    "# review 추출\n",
    "corpus = merged_df['r_comments']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:45.724546Z",
     "start_time": "2021-10-18T08:06:42.459934Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리를 위한 전체 데이터의 특징을 빈도분석으로 파악\n",
    "import nltk\n",
    "\n",
    "# toal tokens 파악\n",
    "total_tokens = [token for msg in corpus for token in str(msg).split()]\n",
    "print(len(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:58.067029Z",
     "start_time": "2021-10-18T08:06:45.726542Z"
    }
   },
   "outputs": [],
   "source": [
    "# 가장 많이 표현된 vocab TOP 10 추출\n",
    "text = nltk.Text(total_tokens, name='NMSC')\n",
    "print(len(set(text.tokens)))\n",
    "print(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:59.578627Z",
     "start_time": "2021-10-18T08:06:58.294420Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print('Unknown system... sorry~~~~')\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "text.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 함수 지정 (text cleaning, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:59.626498Z",
     "start_time": "2021-10-18T08:06:59.581620Z"
    }
   },
   "outputs": [],
   "source": [
    "# MeCab 함수 설정\n",
    "\n",
    "\"\"\"\n",
    "    Parsing 규칙의 문제점, split을 \",\" 기준으로 하는데, token이 \",\" 인 경우에는 쉼표만 잘려서 나오기 때문에, \n",
    "    + \"%,\" 같이 특수문자와 쉼표가 같이 등장하는 경우도 생각해주어야 함.\n",
    "    \n",
    "    (\",\", \"SC\") 의 원래 튜플이 만들어지지 않음.\n",
    "    \n",
    "    명사 분석의 경우 해당 토큰이 필요하지 않으니 pass\n",
    "    \n",
    "    형태소 분석과 POS tagging의 경우 해당 토큰이 필요하므로, token[0]이 ' 인 경우엔 따로 (\",\", \"SC\")를 집어 넣어줘야함.\n",
    "\"\"\"\n",
    "import MeCab # 윈도우 명령어\n",
    "import re\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "def mecab_nouns(text):\n",
    "    nouns = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    # 추출한 token중에 POS가 명사 분류에 속하는 토큰만 선택.\n",
    "    for token in temp:\n",
    "        if token[1] == \"NNG\" or token[1] == \"NNP\" or token[1] == \"NNB\" or token[1] == \"NNBC\" or token[1] == \"NP\" or token[1] == \"NR\":\n",
    "            nouns.append(token[0])\n",
    "        \n",
    "    return nouns\n",
    "\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    # 추출한 token중에 문자열만 선택.\n",
    "    for token in temp:\n",
    "        morphs.append(token[0])\n",
    "    \n",
    "    return morphs\n",
    "\n",
    "def mecab_pos(text):\n",
    "    pos = []\n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    pos = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:06:59.641458Z",
     "start_time": "2021-10-18T08:06:59.628494Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def message_cleaning(docs):\n",
    "\n",
    "    \"\"\"\n",
    "        1. Photo, Emoticon은 내용알기 어려움 -> 제거\n",
    "        \n",
    "        2. 자음/모음 표현 처리방법.\n",
    "            1) \"ㅇㅇ\" ,\"ㅋㅋㅋㅋㅋ\" 같은 자음만 존재하는 표현이나, \"ㅡㅡ\", \"ㅠㅠ\" 같은 모음만 존재하는 표현들은\n",
    "            의미는 있으나 중요한 의미를 가지고 있지 않다고 판단하여 제거.\n",
    "            \n",
    "            2) 이러한 표현들도 전부 emoticon 같은 감정 표현의 의성어로 쓰거나, 단축 표현이므로 제거하지 않음. \n",
    "            \n",
    "        3. http:// 로 시작하는 hyperlink 제거.\n",
    "        \n",
    "        4. 특수문자 제거.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Series의 object를 str로 변경.\n",
    "    docs = [str(doc) for doc in docs]\n",
    "    \n",
    "    # 1. 사진 & 이모티콘 제거 \n",
    "    pattern1 = re.compile(\"사진|Emoticon\")\n",
    "    docs = [pattern1.sub(\"\", doc) for doc in docs]\n",
    "    \n",
    "    # 2. 자음으로만 된 텍스트 & 모음으로만 된 텍스트 제거 (필요시)\n",
    "    pattern2 = re.compile(\"[ㄱ-ㅎ]*[ㅏ-ㅢ]*\")\n",
    "    docs = [pattern2.sub(\"\", doc) for doc in docs]\n",
    "    \n",
    "    # 3. hyperlink 제거 (from. googling)\n",
    "    # http 로 시작하는 경우 : (https?:\\/\\/)\n",
    "    # www.로 시작하는 경우 : ([\\w.]+){1,2}\n",
    "    # www 이후에 .com, .co.kr 등 : (\\.[\\w]{2,4}){1,2}(.*)\n",
    "    pattern3 = re.compile(r\"\\b(https?:\\/\\/)?([\\w.]+){1,2}(\\.[\\w]{2,4}){1,2}(.*)\")\n",
    "    docs = [pattern3.sub(\"\", doc) for doc in docs]\n",
    "    \n",
    "    # 4 특수문자 제거\n",
    "    pattern4 = re.compile(\"[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]\")\n",
    "    docs = [pattern4.sub(\"\", doc) for doc in docs]\n",
    "\n",
    "    return docs\n",
    "\n",
    "# 불용어 추가\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW\n",
    "\n",
    "# 토크나이저 \n",
    "# mecab_nouns / mecab_morphs / mecab_pos 중 택 1\n",
    "def text_tokenizing(doc):\n",
    "    return [word for word in mecab_morphs(doc) if word not in SW and len(word) > 1]\n",
    "    \n",
    "    # wordcloud를 위해 명사만 추출하는 경우.\n",
    "    #return [word for word in mecab.nouns(doc) if word not in SW and len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리뷰 내용 클리닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:25.841872Z",
     "start_time": "2021-10-18T08:06:59.643454Z"
    }
   },
   "outputs": [],
   "source": [
    "SW = define_stopwords(\"stopwords-ko.txt\") # 미리지정된 한국어 불용어 \n",
    "\n",
    "\n",
    "# 카카오톡 텍스트를 정제 (특수문자, 자음&모음 제거, 링크제거, 특수문자 제거)\n",
    "cleaned_corpus = message_cleaning(corpus)\n",
    "print(len(cleaned_corpus)) # 정제된 텍스트 길이 확인\n",
    "print(cleaned_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:26.887077Z",
     "start_time": "2021-10-18T08:07:25.843836Z"
    }
   },
   "outputs": [],
   "source": [
    "# 정제되어 지워지는 텍스트들을 확인하고 분석 대상에서 제외 해야한다. (내용이 없기 때문)\n",
    "\n",
    "cleaned_text = pd.Series(cleaned_corpus) # 정제된 text\n",
    "#merged_df[\"Message\"] = cleaned_text # indexing으로 정제\n",
    "cleaned_data = merged_df[merged_df[\"r_comments\"] != \"\"] # 비어있는 str은 제외\n",
    "cleaned_data.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:26.991763Z",
     "start_time": "2021-10-18T08:07:26.890037Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필요 컬럼만 걸러내기\n",
    "# 상권코드 or 상권코드명을 User로 사용\n",
    "cleaned_data = cleaned_data[['r_date','TRDAR_CD','r_comments']]\n",
    "\n",
    "#결과를 확인\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:27.141364Z",
     "start_time": "2021-10-18T08:07:26.993759Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:29.181196Z",
     "start_time": "2021-10-18T08:07:27.142361Z"
    }
   },
   "outputs": [],
   "source": [
    "# 정제한 리뷰를 pk로 저장\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"cleaned_review.pk\", \"wb\") as f:\n",
    "    pickle.dump(cleaned_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA를 활용한 ATM( Author Topic Model)\n",
    "---\n",
    "- Author = 매장 , 매장별 토픽 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:30.172054Z",
     "start_time": "2021-10-18T08:07:29.183162Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"cleaned_review.pk\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상권별 데이터 추려내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:30.354536Z",
     "start_time": "2021-10-18T08:07:30.174019Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 사용자별로 데이터 처리를 해야 함\n",
    "# 사용자 데이터 추려내기\n",
    "users = set(data[\"TRDAR_CD\"])\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:30.891101Z",
     "start_time": "2021-10-18T08:07:30.356531Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# User별로 groupby로 묶는다.\n",
    "authors = data.groupby('TRDAR_CD')\n",
    "\n",
    "# 결과 확인\n",
    "pprint(authors.groups)\n",
    "print(type(authors.groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:31.251138Z",
     "start_time": "2021-10-18T08:07:30.895090Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# groupby로 묶인 User 데이터를 Int64Index에서 list로 변경해준다.\n",
    "\n",
    "author2doc = {} \n",
    "\n",
    "for user, index in authors.groups.items():\n",
    "    author2doc[user] = list(index)\n",
    "    \n",
    "print(author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:35.923903Z",
     "start_time": "2021-10-18T08:07:31.258119Z"
    }
   },
   "outputs": [],
   "source": [
    "# gensim에 들어갈 데이터를 생성\n",
    "# LDA의 intput : [corpus, dictionary, topic개수] 등\n",
    "# ATM도 마찬가지\n",
    "\n",
    "# Message 컬럼의 각 row는 string -> 이것들이 tokenized된 것이 corpus\n",
    "# gensim의 corpus와 dictionary를 만들려면 list of list of word 형태의 데이터가 있어야 한다.\n",
    "# Message컬럼의 값을 list로 만들고, 하나 하나 불러와서 split() 해준다.\n",
    "tokenized_data = [str(msg).split() for msg in list(data[\"r_comments\"])]\n",
    "print(tokenized_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim으로 ATM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:07:36.299405Z",
     "start_time": "2021-10-18T08:07:35.926896Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.models import CoherenceModel\n",
    "# bleicorpus : 결과 저장할때 쓰는 포멧\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.matutils import hellinger\n",
    "from gensim import corpora\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:08:26.852710Z",
     "start_time": "2021-10-18T08:07:36.300403Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ATM에 사용할 Dictionary 만들기 (기본적으로 LDA와 동일)\n",
    "corpus, dictionary는 데이터가 클수록 한번 만들때 오랜 시간이 소요 된다.\n",
    "동일 데이터를 여러번 활용할것이라면 한번 만들었을때 저장해 두는 것이 좋다.\n",
    "\"\"\"\n",
    "\n",
    "# ATM에 사용할 Dictionary 만들기\n",
    "# 해당 파일이 없으면 생성\n",
    "if not os.path.exists('review(ATM)_dict'):\n",
    "    # 토큰화한 데이터를 dictionary화 \n",
    "    dictionary = corpora.Dictionary(tokenized_data)\n",
    "    dictionary.save('review(ATM)_dict') # 저장\n",
    "    print(dictionary)\n",
    "# 해당 파일이 존재하면 load\n",
    "else:\n",
    "    dictionary = Dictionary.load('review(ATM)_dict')\n",
    "\n",
    "# ATM에 사용할 corpus 만들기\n",
    "if not os.path.exists('review(ATM)_corpus'):\n",
    "    # tokenized_data의 각 row를 BOW로 변형\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]\n",
    "    corpora.BleiCorpus.serialize('review(ATM)_corpus', corpus)\n",
    "else:\n",
    "    corpus = bleicorpus.BleiCorpus('review(ATM)_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:08:26.867672Z",
     "start_time": "2021-10-18T08:08:26.856700Z"
    }
   },
   "outputs": [],
   "source": [
    "# ATM에 들어갈 데이터 확인\n",
    "# 단어 개수, 문서개수, 저자의 수\n",
    "print('매장 수: %d' % len(authors))\n",
    "print('Unique한 토큰의 수: %d' % len(dictionary))\n",
    "print('총 리뷰 수: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:09:40.972791Z",
     "start_time": "2021-10-18T08:08:26.901580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dictionary함수로 만든 사전에 있는 단어 보기\n",
    "# 벡터화 한 곳에 텍스트가 들어갈 수 없으니 사전에 할당된 각 index값이 들어간다.\n",
    "print(dictionary)\n",
    "for idx in dictionary:\n",
    "    print(dictionary[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-18T08:09:49.674187Z",
     "start_time": "2021-10-18T08:09:40.977777Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 사람이 이해할 수 있는 형태로 코퍼스 사전 재구성 (term-frequency) : 이 방식을 활용해 ATM모델링에 적용\n",
    "[[(dictionary[id], freq) for id, freq in cp] for cp in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATM 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Author Topic Model 실행\n",
    "if not os.path.exists(\"review(ATM)_model\"):\n",
    "    # 모델이 없으면 생성\n",
    "    # num_topics는 default가 100 (5 개로 지정)\n",
    "    # author2doc : 저자가 어떤 문서를 썼는가에 대한 index가 있는 dict\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=5, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, passes=10)\n",
    "    model.save('review(ATM)_model')\n",
    "else:\n",
    "    model = AuthorTopicModel.load(\"review(ATM)_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.076Z"
    }
   },
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "model.show_topic(0, topn=20) # 0번째 topic의 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATM  모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.079Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim import corpora\n",
    "from tqdm import tqdm_notebook\n",
    "from pprint import pprint\n",
    "\n",
    "# 사용자간의 유사성을 평가하기 위한 measure를 사용하기 위해 불러오기\n",
    "from gensim.matutils import hellinger\n",
    "from gensim import matutils\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.081Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "\n",
    "if not os.path.exists(\"review(ATM)_model\"):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary.id2token, \\\n",
    "                author2doc=author2doc, passes=10)\n",
    "    model.save('review(ATM)_model')\n",
    "else:\n",
    "    model = AuthorTopicModel.load(\"review(ATM)_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.086Z"
    }
   },
   "outputs": [],
   "source": [
    "# 토픽 라벨 지정.\n",
    "topic_labels = [\"Topic0\", \"Topic1\", \"Topic2\", \"Topic3\", \"Topic4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.089Z"
    }
   },
   "outputs": [],
   "source": [
    "# 토픽별로 topN 단어 확인하기.\n",
    "for topic in model.show_topics(NUM_TOPICS):\n",
    "    print('Label: ' + topic_labels[topic[0]])\n",
    "    words = ''\n",
    "    for word, prob in model.show_topic(topic[0], topn=20):\n",
    "        words += word + ' '\n",
    "    print('Words: ' + words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.091Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용자별로 토픽 분포 확인하기.\n",
    "def show_store(name):\n",
    "    print('User: ',  name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"카페거실\"의 토픽 분포 확인. (어느 토픽에 속할 확률이 가장 높은가)\n",
    "# 추후 상권 단위 or 클러스터 로 묶는다면 단위별 토픽 분포 확인도 가능\n",
    "show_store('카페거실')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hellinger Distance를 개별로 구하기 위해 각 author(sotre)가 다른 authors간의 probability distributions를 도출\n",
    "# 아래의 함수에 들어갈 예정\n",
    "#[model.get_author_topics(author) for author in model.id2author.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hellinger Distance를 이용하여 비슷한 토픽을 가진 사용자(store)를 추정하는 함수.\n",
    "# Hellinger Distance : 두개의  probability distributions 사이의 거리를 재는 방법\n",
    "# store별 topic distribution이 들어간 df가 필요\n",
    "\n",
    "\n",
    "# author-topic 분포 만들기\n",
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]\n",
    " \n",
    "def similarity(vec1, vec2):\n",
    "    # vec1, vec2 사이의 hellinger similarity 구하기.\n",
    "    dist = hellinger(matutils.sparse2full(vec1, model.num_topics), \\\n",
    "                              matutils.sparse2full(vec2, model.num_topics))\n",
    "    # simulate : 1 / distance + 1  (smooth 형식)\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    " \n",
    "def get_sims(vec):\n",
    "    # 각 사용자들 사이의 similarity pair 구하기.\n",
    "    sims = [similarity(vec, vec2) for vec2 in author_vecs]\n",
    "    return sims\n",
    " \n",
    "def get_table(name, top_n=10, smallest_author=1):\n",
    "    \"\"\"\n",
    "    주어진 사용자에 대해서 topN 사람만큼 유사도를 \n",
    "    정렬해서 table을 출력하는 함수\n",
    "    top_n : 기본 10개\n",
    "    smallest_author : review수가 지정된 값 이하이면 제외\n",
    "    \"\"\"\n",
    "    \n",
    "    # 유사도 측정하기\n",
    "    # model.get_author_topics(name) : 각 author와 다른 authors 간의 distribution\n",
    "    sims = get_sims(model.get_author_topics(name))\n",
    " \n",
    "    # 저자별 정보 정렬하기\n",
    "    # 비슷한 정도를 기준으로 정렬\n",
    "    table = []\n",
    "    for elem in enumerate(sims):\n",
    "        author_name = model.id2author[elem[0]]\n",
    "        sim = elem[1]\n",
    "        author_size = len(model.author2doc[author_name])\n",
    "        if author_size >= smallest_author:\n",
    "            table.append((author_name, sim, author_size))\n",
    "            \n",
    "    # 사용자 패턴 분석 결과를 Dataframe으로 만들기\n",
    "    # score = 유사도\n",
    "    # size = review의 수\n",
    "    df = pd.DataFrame(table, columns=['Store_Name', 'Score', 'Size'])\n",
    "    df = df.sort_values('Score', ascending=False)[:top_n] # score순으로 정렬\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.103Z"
    }
   },
   "outputs": [],
   "source": [
    "# 노원구 안에 있는 각 식당 = Author\n",
    "# 제가 원래 하고 싶 었던 거는 -> 각 상권 =Author\n",
    "\n",
    "\n",
    "# 10000개 -> 상권 코드 -> 코드별로 리뷰 수집 = corpus \n",
    "# 상권별 review -> ATM 분석 -> 유사상권 get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-18T08:06:35.116Z"
    }
   },
   "outputs": [],
   "source": [
    "# 매장별 리뷰에 대한 유사도 검증\n",
    "get_table('제임스키친') # 상권A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- owner review : 각 상권에 owner review 가 어떤 영향을 미치는지\n",
    "    - 상권 기준 owner revew /전체 리뷰 비율 -> 회귀 feature로 사용\n",
    "    \n",
    "y : 상권 매출 회귀 계수\n",
    "x : 맛에 대한 표현 비율 , 분위기에 대한 표현 비율, 가성비에 대한 표현 비율, owner의 소통력(적극성지표), 평균 평점, 평균 리뷰수,\n",
    "\n",
    " 1이하 평점수의 비율 => 1~2 는 부정 평가 -> 부정 평가 비율\n",
    "3~5는 긍정평가  -> 긍정평가 비율\n",
    "\n",
    "회귀 -> 영향력 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textmining]",
   "language": "python",
   "name": "conda-env-textmining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
